{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "45e3a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import librosa_trans\n",
    "import re\n",
    "from IPython.display import clear_output, display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a986c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_path = os.path.abspath('/media/yiying/TOS/')\n",
    "test_list = ['normal','queenless']\n",
    "\n",
    "data=pd.DataFrame()\n",
    "\n",
    "for test in test_list:\n",
    "    dir_path = sorted(os.listdir(os.path.join(sound_path, test)))\n",
    "    for file in dir_path:\n",
    "        time_detail = re.search( r'(.*).wav', file).group(1)\n",
    "        date,time = time_detail.split('-')\n",
    "        date = date.split('_')[-1]\n",
    "        time_split = datetime.strptime(date+time,'%Y%m%d%H%M%S')\n",
    "        data=data.append({'date':time_split,'path':os.path.join(sound_path,test,file),'id':'Yilan_queenless4','district':'','state':test}, ignore_index=True)\n",
    "        \n",
    "data.to_csv('/media/yiying/TOS/datalist.csv',index_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ac777193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length: 319\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>district</th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>314</td>\n",
       "      <td>2021-09-04 17:16:18</td>\n",
       "      <td>east</td>\n",
       "      <td>Yilan_queenless4</td>\n",
       "      <td>/media/yiying/TOS/queenless/20210904-171618.wav</td>\n",
       "      <td>queenless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>315</td>\n",
       "      <td>2021-09-04 17:17:28</td>\n",
       "      <td>east</td>\n",
       "      <td>Yilan_queenless4</td>\n",
       "      <td>/media/yiying/TOS/queenless/20210904-171728.wav</td>\n",
       "      <td>queenless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>316</td>\n",
       "      <td>2021-09-04 17:18:38</td>\n",
       "      <td>east</td>\n",
       "      <td>Yilan_queenless4</td>\n",
       "      <td>/media/yiying/TOS/queenless/20210904-171838.wav</td>\n",
       "      <td>queenless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>317</td>\n",
       "      <td>2021-09-04 17:19:48</td>\n",
       "      <td>east</td>\n",
       "      <td>Yilan_queenless4</td>\n",
       "      <td>/media/yiying/TOS/queenless/20210904-171948.wav</td>\n",
       "      <td>queenless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>318</td>\n",
       "      <td>2021-09-04 17:20:58</td>\n",
       "      <td>east</td>\n",
       "      <td>Yilan_queenless4</td>\n",
       "      <td>/media/yiying/TOS/queenless/20210904-172058.wav</td>\n",
       "      <td>queenless</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                 date district                id  \\\n",
       "314         314  2021-09-04 17:16:18     east  Yilan_queenless4   \n",
       "315         315  2021-09-04 17:17:28     east  Yilan_queenless4   \n",
       "316         316  2021-09-04 17:18:38     east  Yilan_queenless4   \n",
       "317         317  2021-09-04 17:19:48     east  Yilan_queenless4   \n",
       "318         318  2021-09-04 17:20:58     east  Yilan_queenless4   \n",
       "\n",
       "                                                path      state  \n",
       "314  /media/yiying/TOS/queenless/20210904-171618.wav  queenless  \n",
       "315  /media/yiying/TOS/queenless/20210904-171728.wav  queenless  \n",
       "316  /media/yiying/TOS/queenless/20210904-171838.wav  queenless  \n",
       "317  /media/yiying/TOS/queenless/20210904-171948.wav  queenless  \n",
       "318  /media/yiying/TOS/queenless/20210904-172058.wav  queenless  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the metadata from the generated CSV\n",
    "metadata = pd.read_csv('/media/yiying/TOS/datalist.csv')\n",
    "\n",
    "# Examine dataframe\n",
    "print(\"Metadata length:\", len(metadata))\n",
    "metadata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "52618d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 318/319\n",
      "Last file:  /media/yiying/TOS/queenless/20210904-172058.wav\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "colonies = []\n",
    "frames_max = 0\n",
    "counter = 0\n",
    "total_samples = len(metadata)\n",
    "n_mfcc = 40\n",
    "\n",
    "# transform to MFCC\n",
    "for index, row in metadata.iterrows():\n",
    "\n",
    "\n",
    "    file_path = os.path.abspath(str(row[\"path\"]))\n",
    "    colony_label = row[\"id\"]\n",
    "    state_label = row[\"state\"]\n",
    "\n",
    "    # Extract MFCCs (do not add padding)\n",
    "    mfccs = librosa_trans.mfcc_transform(file_path)\n",
    "\n",
    "    # Save current frame count\n",
    "    num_frames = mfccs.shape[1]\n",
    "\n",
    "    # Add row (feature / label)\n",
    "    features.append(mfccs)\n",
    "    labels.append(state_label)\n",
    "    colonies.append(colony_label)\n",
    "\n",
    "    # Update frames maximum\n",
    "    if (num_frames > frames_max):\n",
    "        frames_max = num_frames\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Progress: {}/{}\".format(index,total_samples))\n",
    "    print(\"Last file: \", file_path)\n",
    "\n",
    "\n",
    "# padding to max_length\n",
    "\n",
    "padded = []\n",
    "\n",
    "# Add padding\n",
    "mfcc_max_padding = frames_max\n",
    "for j in range(len(features)):\n",
    "    size = len(features[j][0])\n",
    "    if (size < mfcc_max_padding):\n",
    "        pad_width = mfcc_max_padding - size\n",
    "        px = np.pad(features[j], \n",
    "                    pad_width=((0, 0), (0, pad_width)), \n",
    "                    mode='constant', \n",
    "                    constant_values=(0,))\n",
    "    else:\n",
    "        px = features[j]\n",
    "\n",
    "    padded.append(px)\n",
    "\n",
    "\n",
    "# save file\n",
    "\n",
    "X = np.array(padded)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Optionally save the features to disk\n",
    "np.save(f\"./data/Yilan-q-4-X-mfcc-augmented\", X)\n",
    "np.save(f\"./data/Yilan-q-4-y-mfcc-augmented\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91c5f27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5f786ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "# print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "66cf1079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 ... -1.0000000e+00\n",
      "   -1.0000000e+00 -1.0000000e+00]\n",
      "  [ 3.2259524e-01  3.3138469e-01  3.3090383e-01 ...  3.4143329e-01\n",
      "    3.3608040e-01  3.2647151e-01]\n",
      "  [ 1.3556598e-01  1.3633381e-01  1.4121857e-01 ...  1.3879907e-01\n",
      "    1.3399698e-01  1.3289391e-01]\n",
      "  ...\n",
      "  [-1.1801712e-02 -9.5799053e-03 -1.1268334e-02 ... -1.5293019e-02\n",
      "   -9.2151053e-03 -5.7651279e-03]\n",
      "  [ 1.1316449e-03  7.4460852e-04 -7.9328669e-03 ... -9.6429177e-03\n",
      "    1.6636715e-03  1.2104601e-02]\n",
      "  [-9.9100508e-03 -7.7228970e-04  3.1809206e-03 ...  1.7828937e-03\n",
      "    7.6698898e-03  1.3510254e-02]]\n",
      "\n",
      " [[-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 ... -1.0000000e+00\n",
      "   -1.0000000e+00 -1.0000000e+00]\n",
      "  [ 3.0290309e-01  3.1254244e-01  3.2148051e-01 ...  3.2798216e-01\n",
      "    3.2205033e-01  3.1877950e-01]\n",
      "  [ 1.4110005e-01  1.4812668e-01  1.4018364e-01 ...  1.4237869e-01\n",
      "    1.4523640e-01  1.4995725e-01]\n",
      "  ...\n",
      "  [-9.3802046e-03 -8.4800785e-03 -1.3300611e-02 ... -6.4719091e-03\n",
      "   -7.6948898e-03 -9.4102668e-03]\n",
      "  [ 3.5214645e-03  1.2270134e-02  7.5012948e-03 ... -2.8754184e-03\n",
      "   -1.3163111e-03 -1.1641784e-02]\n",
      "  [-1.5255128e-02 -8.9245830e-03 -3.1818098e-03 ... -2.3210561e-03\n",
      "   -5.2208607e-03 -1.2502868e-02]]\n",
      "\n",
      " [[-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 ... -1.0000000e+00\n",
      "   -1.0000000e+00 -1.0000000e+00]\n",
      "  [ 3.0041751e-01  3.1674799e-01  3.2533595e-01 ...  3.4276810e-01\n",
      "    3.3620182e-01  3.1712195e-01]\n",
      "  [ 1.4788404e-01  1.3957033e-01  1.3608624e-01 ...  1.4982453e-01\n",
      "    1.4523213e-01  1.4462869e-01]\n",
      "  ...\n",
      "  [-4.2063235e-03 -7.0531010e-03 -7.9185152e-03 ... -6.4675873e-03\n",
      "   -8.3259343e-05  8.9936526e-03]\n",
      "  [ 1.2450373e-02 -2.3586573e-03 -1.2880140e-03 ...  5.9950897e-03\n",
      "    7.4513676e-03  1.1282811e-02]\n",
      "  [-1.8126354e-02 -1.9550823e-02 -6.3540586e-03 ... -6.4837844e-03\n",
      "   -5.1284973e-03 -3.6629748e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 ... -1.0000000e+00\n",
      "   -1.0000000e+00 -1.0000000e+00]\n",
      "  [ 3.3304736e-01  3.4531987e-01  3.3665282e-01 ...  3.3788717e-01\n",
      "    3.4028462e-01  3.2717937e-01]\n",
      "  [ 1.8308049e-01  1.6642328e-01  1.5051471e-01 ...  1.4176279e-01\n",
      "    1.4576341e-01  1.4873224e-01]\n",
      "  ...\n",
      "  [-1.3827448e-02 -9.4892513e-03 -5.6944573e-03 ... -3.9071441e-03\n",
      "    6.2694172e-03  8.1113195e-03]\n",
      "  [-4.4001611e-03 -2.1557470e-03 -1.3204910e-02 ... -1.0071996e-02\n",
      "   -2.6201629e-03  2.3244191e-03]\n",
      "  [-1.2937279e-02 -8.7209744e-03 -1.0149847e-02 ... -2.2074052e-03\n",
      "    2.6206775e-03  4.8694829e-03]]\n",
      "\n",
      " [[-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 ... -1.0000000e+00\n",
      "   -1.0000000e+00 -1.0000000e+00]\n",
      "  [ 3.3804974e-01  3.4285009e-01  3.4348455e-01 ...  3.2488951e-01\n",
      "    3.2718235e-01  3.2706404e-01]\n",
      "  [ 1.2971531e-01  1.3516821e-01  1.4801943e-01 ...  1.3409968e-01\n",
      "    1.4581412e-01  1.5018195e-01]\n",
      "  ...\n",
      "  [-2.1492066e-03 -8.9847483e-03 -9.5413001e-03 ... -5.2008596e-03\n",
      "    1.0876259e-04 -3.1252735e-04]\n",
      "  [-1.0998363e-02 -1.6921824e-02 -1.2904367e-02 ... -1.1201092e-02\n",
      "   -7.3740077e-03 -1.0833157e-03]\n",
      "  [-2.0380900e-03 -8.8581834e-03 -1.5553609e-03 ... -6.8240897e-03\n",
      "   -6.9128801e-03 -1.8164977e-04]]\n",
      "\n",
      " [[-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 ... -1.0000000e+00\n",
      "   -1.0000000e+00 -1.0000000e+00]\n",
      "  [ 3.2722801e-01  3.4538332e-01  3.4094253e-01 ...  3.4175238e-01\n",
      "    3.2768217e-01  3.2599342e-01]\n",
      "  [ 1.6516297e-01  1.5270580e-01  1.3308413e-01 ...  1.4354269e-01\n",
      "    1.2924159e-01  1.3202041e-01]\n",
      "  ...\n",
      "  [-2.3305569e-02 -7.6940479e-03  1.1793049e-02 ...  5.9962552e-03\n",
      "    4.2002434e-03 -7.4115320e-04]\n",
      "  [-2.0738240e-02 -8.0044540e-03 -3.5354956e-03 ... -4.5138872e-03\n",
      "   -7.5863432e-03 -1.7899541e-02]\n",
      "  [ 6.9866504e-04  2.1128275e-04  1.0733028e-03 ... -2.4009277e-03\n",
      "    1.2349014e-03  3.9439402e-03]]]\n",
      "['normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd' 'imd'\n",
      " 'imd' 'imd']\n"
     ]
    }
   ],
   "source": [
    "def concat_dataset():\n",
    "    path = './data/'\n",
    "    types = ['normal', 'imd']\n",
    "    features = labels = np.array([])\n",
    "    \n",
    "\n",
    "                \n",
    "    features_filename = f'Yilan-imd1-X-mfcc-augmented.npy'\n",
    "    labels_filename = f'Yilan-imd1-y-mfcc-augmented.npy'\n",
    "\n",
    "    curr_features = np.load(os.path.join(path, features_filename))\n",
    "    curr_labels = np.load(os.path.join(path, labels_filename))\n",
    "\n",
    "    if features.size == 0:\n",
    "        features = curr_features\n",
    "    else:\n",
    "        features = np.concatenate((features, curr_features))\n",
    "\n",
    "    if labels.size == 0:\n",
    "        labels = curr_labels\n",
    "    else:\n",
    "        labels = np.concatenate((labels, curr_labels))\n",
    "    \n",
    "    return (features, labels)\n",
    "\n",
    "        \n",
    "dataset = concat_dataset()\n",
    "print(dataset[0])\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30784fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "b5b169ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def transfer(x):\n",
    "    if x=='normal':\n",
    "        return 2\n",
    "    if x=='flu':\n",
    "        return 0\n",
    "    if x=='imd':\n",
    "        return 1\n",
    "    if x=='queenless':\n",
    "        return 3\n",
    "\n",
    "\n",
    "features = dataset[0]\n",
    "labels = np.fromiter(map(transfer, dataset[1]), dtype=np.int) \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "22196b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "17/17 [==============================] - 1s 6ms/step - loss: 5.2875 - accuracy: 0.0000e+00\n",
      "test loss, test acc: [5.287547588348389, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('./model/multiclass_model_q_yunlinankeng.h5')\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(features, labels)#, batch_size=128\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "e22832f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 3 0 3 0 3 3 0 3 0 0 0 0 3 0 0 3 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0\n",
      " 3 3 0 0 3 3 3 3 3 3 0 3 0 0 3 0 3 3 0 3 0 3 0 0 3 3 3 3 3 3 3 3 3 0 3 3 3\n",
      " 0 0 3 3 0 3 0 0 3 3 0 3 3 0 0 3 0 3 0 3 3 3 3 0 0 0 0 0 3 0 0 0 0 0 3 3 0\n",
      " 0 0 3 3 0 0 0 0 0 3 0 3 0 3 0 3 0 0 0 3 3 3 3 0 0 0 3 3 0 0 0 0 0 0 3 0 0\n",
      " 0 0 3 3 0 0 0 0 3 0 0 3 0 3 0 0 0 0 0 3 0 3 3 0 0 3 0 0 0 0 3 3 0 0 3 3 3\n",
      " 0 3 3 0 0 0 0 0 0 0 3 0 0 3 3 3 3 3 3 0 0 0 0 0 0 3 0 0 0 0 0 0 3 0 0 0 0\n",
      " 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 3 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 0 3 3 3\n",
      " 3 3 3 3 3 0 0 0 0 0 0 3 0 0 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 2 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 3 2 2 2 3 2 2 2 2 3 3 2 3\n",
      " 3 3 3 2 2 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 2 3 3 3 2 2 3 3 3 2 3 2 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3\n",
      " 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(features)\n",
    "classes = np.argmax(predictions, axis=1)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca13142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
